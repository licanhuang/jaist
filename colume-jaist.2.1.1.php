<h3>Journal of advances in information science and technology</h3>
</br>
<a href="jaist_2_1.php">Volume 2, Issue 1</a>, January 2024,  Pages: 1 - 10
</br>
</br>
<p style ="font-size:18px; color: blue;">Research on Interpretable Machine Learning Portfolio 
Based on Multi-factor Clustering  </p>
</br>
<a href="2/1/jaist.2.1.1.pdf">Download article (PDF)</a>

</br>
</br>
<p style ="font-size:8px;"><strong>DOI</strong></p>
<p><a href="https://doi.org/10.5281/zenodo.10545042">10.5281/zenodo.10545042 </a></p>
</br>

<p style ="font-size:8px;"><strong>Authors</strong></p>
<strong>JiHui Shi<sup>1</sup>
, WenZheng Zhang<sup>2</sup>  </strong>
</br>
1 School of Economic Management, Huzhou University, Huzhou 
Zhejiang, China  
</br>
2 School of Information Engineering, Huzhou 
University, Huzhou Zhejiang China
</br>

</br>
</br>
<p style ="font-size:8px;"><strong>Corresponding Author:</strong></p>
 
JiHui Shi  (e-mail: jacky-shi@zjhu.edu.cn)

</br>
</br>
<p style ="font-size:8px;">Received 10 December 2023, Accepted 16 Janaury 2024, Available Online 21 January 2024.</p>
</br></br>

<p style ="font-size:8px;"><strong>Keywords</strong></p>

Interpretability, Multi-factor Mode, Stock 
Clustering, Random Forest, Portfolio.

</br>
</br>
<p style ="font-size:8px;"><strong>Abstract</strong></p>
The 'black box' phenomenon and limited 
interpretability present significant obstacles in machine 
learning and deep learning for portfolio management. 
Additionally, standard metrics for interpretability in machine 
learning often struggle to effectively elucidate model features in 
portfolio decision contexts. This research aims to address these 
challenges by introducing a methodology for generating easily 
interpretable portfolios. The approach involves using Random 
Forest feature importance analysis within multi-factor models, 
followed by clustering based on stock factors. Portfolios are 
generated using the Mean-CVaR model, and the effectiveness of 
the proposed explainable portfolios is evaluated through 
comparative analysis with two machine learning interpretability 
tools: SHAP and Permutation methods.
</br>

<p style ="font-size:8px;"><strong>Copyright</strong></p>


 @2023 The Authors. Published by Research Institute of Information Technology (Tokyo office), Hangzhou Domain Zones Technology Co., Ltd.
</br>
</br>

<p style ="font-size:8px;"><strong>Open Access</strong></p>

This is an open access article distributed under the CC BY-NC 4.0 
</br>
license CC BY-NC 4.0 : <a href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/ </a>.

